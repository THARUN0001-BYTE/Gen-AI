{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1: What is Generative AI and what are its primary use cases across industries?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Generative AI is a branch of Artificial Intelligence that focuses on creating new content—such as text, images, audio, video, code, or synthetic data—by learning patterns from existing data.\n",
        "Unlike traditional AI (which mainly analyzes or predicts), Generative AI produces original outputs that resemble human-created content."
      ],
      "metadata": {
        "id": "wORkZ98UISEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hb4z9POvIyt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Explain the role of probabilistic modeling in generative models. How do these models differ from discriminative models?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Role of Probabilistic Modeling in Generative Models\n",
        "\n",
        "Probabilistic modeling is the foundation of generative models. It allows these models to learn the underlying probability distribution of data so they can generate new, realistic samples.\n",
        "\n",
        "Captures uncertainty\n",
        "Real-world data is noisy. Probabilistic models represent uncertainty instead of giving fixed outputs.\n",
        "\n",
        "Enables data generation\n",
        "Once the distribution is learned, the model can sample new data points.\n",
        "\n",
        "Handles missing data well\n",
        "Since the full distribution is known, missing values can be inferred.\n",
        "\n",
        "Supports Bayesian reasoning\n",
        "Models can update beliefs as new data arrives."
      ],
      "metadata": {
        "id": "W_8KfZY1IzkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "f4X-OjSXJKp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3: What is the difference between Autoencoders and Variational\n",
        "Autoencoders (VAEs) in the context of text generation?\n",
        "\n",
        "Answers:\n",
        "\n",
        "\n",
        "Difference Between Autoencoders (AEs) and Variational Autoencoders (VAEs) in Text Generation\n",
        "\n",
        "Autoencoders and Variational Autoencoders are both encoder–decoder architectures, but they differ fundamentally in how they represent latent space and generate text."
      ],
      "metadata": {
        "id": "6oUD-WYFJLfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "q6x9ov-_JgFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4: Describe the working of attention mechanisms in Neural Machine\n",
        "Translation (NMT). Why are they critical?\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "Working of Attention Mechanisms in Neural Machine Translation (NMT) & Why They Are Critical\n",
        "\n",
        "Attention mechanisms were introduced to overcome key limitations of early encoder–decoder NMT models and are now central to modern translation systems (including Transformers)."
      ],
      "metadata": {
        "id": "CnA3QkPnJg4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bR7WuYh5KegI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: What ethical considerations must be addressed when using generative AI\n",
        "for creative content such as poetry or storytelling?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Ethical Considerations in Using Generative AI for Creative Content (Poetry & Storytelling)\n",
        "\n",
        "When Generative AI is used for creative domains like poetry or storytelling, it raises important ethical, legal, and social concerns beyond technical performance.\n",
        "\n",
        "Authorship & Intellectual Property (IP)\n",
        "\n",
        "Who owns AI-generated poems or stories—the user, the developer, or no one?\n",
        "\n",
        "Risk of unintentional plagiarism if training data includes copyrighted works\n",
        "\n",
        "AI may reproduce styles too closely to specific authors\n",
        "\n",
        "AI can imitate famous writers’ styles (e.g., Shakespeare, Tagore)\n",
        "\n",
        "Blurred line between creative inspiration and copying\n",
        "\n",
        "Training data may contain:\n",
        "\n",
        "Cultural bias\n",
        "\n",
        "Gender stereotypes\n",
        "\n",
        "Ethnic misrepresentation\n",
        "\n",
        "Creative narratives can reinforce harmful tropes"
      ],
      "metadata": {
        "id": "VO3jW1BjKfgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pwd4IAtfK_EQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6: Use the following small text dataset to train a simple Variational\n",
        "Autoencoder (VAE) for text reconstruction:\n",
        "[\"The sky is blue\", \"The sun is bright\", \"The grass is green\",\n",
        "\"The night is dark\", \"The stars are shining\"]\n",
        "1. Preprocess the data (tokenize and pad the sequences).\n",
        "2. Build a basic VAE model for text reconstruction.\n",
        "3. Train the model and show how it reconstructs or generates similar sentences.\n",
        "Include your code, explanation, and sample outputs.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "8ihgDpX-LAuI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_01cwPZgHmKi"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1. Imports\n",
        "# =========================\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers, Model, backend as K\n",
        "\n",
        "# =========================\n",
        "# 2. Dataset\n",
        "# =========================\n",
        "sentences = [\n",
        "    \"the sky is blue\",\n",
        "    \"the sun is bright\",\n",
        "    \"the grass is green\",\n",
        "    \"the night is dark\",\n",
        "    \"the stars are shining\"\n",
        "]\n",
        "\n",
        "# =========================\n",
        "# 3. Tokenization & Padding\n",
        "# =========================\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# =========================\n",
        "# 4. VAE Parameters\n",
        "# =========================\n",
        "embedding_dim = 16\n",
        "latent_dim = 8\n",
        "\n",
        "# =========================\n",
        "# 5. Encoder\n",
        "# =========================\n",
        "inputs = layers.Input(shape=(max_len,))\n",
        "x = layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
        "x = layers.LSTM(32)(x)\n",
        "\n",
        "z_mean = layers.Dense(latent_dim)(x)\n",
        "z_log_var = layers.Dense(latent_dim)(x)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z])\n",
        "\n",
        "# =========================\n",
        "# 6. Decoder\n",
        "# =========================\n",
        "latent_inputs = layers.Input(shape=(latent_dim,))\n",
        "x = layers.RepeatVector(max_len)(latent_inputs)\n",
        "x = layers.LSTM(32, return_sequences=True)(x)\n",
        "outputs = layers.TimeDistributed(\n",
        "    layers.Dense(vocab_size, activation='softmax')\n",
        ")(x)\n",
        "\n",
        "decoder = Model(latent_inputs, outputs)\n",
        "\n",
        "# =========================\n",
        "# 7. VAE Model\n",
        "# =========================\n",
        "vae_outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, vae_outputs)\n",
        "\n",
        "# =========================\n",
        "# 8. Loss Function\n",
        "# =========================\n",
        "reconstruction_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "    X, vae_outputs\n",
        ")\n",
        "reconstruction_loss = K.mean(reconstruction_loss)\n",
        "\n",
        "kl_loss = -0.5 * K.mean(\n",
        "    1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        ")\n",
        "\n",
        "vae.add_loss(reconstruction_loss + kl_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "# =========================\n",
        "# 9. Training\n",
        "# =========================\n",
        "vae.fit(X, X, epochs=300, verbose=0)\n",
        "\n",
        "# =========================\n",
        "# 10. Reconstruction\n",
        "# =========================\n",
        "preds = vae.predict(X)\n",
        "\n",
        "def decode_sentence(pred):\n",
        "    words = []\n",
        "    for timestep in pred:\n",
        "        idx = np.argmax(timestep)\n",
        "        word = tokenizer.index_word.get(idx, \"\")\n",
        "        words.append(word)\n",
        "    return \" \".join(words)\n",
        "\n",
        "print(\"\\nOriginal vs Reconstructed:\\n\")\n",
        "for i in range(len(sentences)):\n",
        "    print(\"Original     :\", sentences[i])\n",
        "    print(\"Reconstructed:\", decode_sentence(preds[i]))\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HysocfkEMmg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7: Use a pre-trained GPT model (like GPT-2 or GPT-3) to translate a short English paragraph into French and German. Provide the original and translated text. (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "9xYMQSppMmdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 1. Install & Import\n",
        "# =========================\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# =========================\n",
        "# 2. Load Pre-trained GPT-2\n",
        "# =========================\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# =========================\n",
        "# 3. Translation Prompts\n",
        "# =========================\n",
        "english_text = (\n",
        "    \"Artificial intelligence is transforming the world by improving \"\n",
        "    \"efficiency and enabling new innovations.\"\n",
        ")\n",
        "\n",
        "prompt_french = f\"Translate English to French:\\nEnglish: {english_text}\\nFrench:\"\n",
        "prompt_german = f\"Translate English to German:\\nEnglish: {english_text}\\nGerman:\"\n",
        "\n",
        "# =========================\n",
        "# 4. Generate Translation\n",
        "# =========================\n",
        "def generate_translation(prompt):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=100,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "french_translation = generate_translation(prompt_french)\n",
        "german_translation = generate_translation(prompt_german)\n",
        "\n",
        "print(\"Original English:\\n\", english_text)\n",
        "print(\"\\nFrench Translation:\\n\", french_translation)\n",
        "print(\"\\nGerman Translation:\\n\", german_translation)\n"
      ],
      "metadata": {
        "id": "Vh0IcAT-MyBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "p2Cc7JJEM2do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8: Implement a simple attention-based encoder-decoder model for\n",
        "English-to-Spanish translation using Tensorflow or PyTorch.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "09gKeJGTM3Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Dataset\n",
        "eng_sentences = [\n",
        "    \"hello\",\n",
        "    \"how are you\",\n",
        "    \"i am fine\",\n",
        "    \"thank you\",\n",
        "    \"good night\"\n",
        "]\n",
        "\n",
        "spa_sentences = [\n",
        "    \"<start> hola <end>\",\n",
        "    \"<start> como estas <end>\",\n",
        "    \"<start> estoy bien <end>\",\n",
        "    \"<start> gracias <end>\",\n",
        "    \"<start> buenas noches <end>\"\n",
        "]\n",
        "# Tokenizers\n",
        "eng_tokenizer = Tokenizer()\n",
        "spa_tokenizer = Tokenizer()\n",
        "\n",
        "eng_tokenizer.fit_on_texts(eng_sentences)\n",
        "spa_tokenizer.fit_on_texts(spa_sentences)\n",
        "\n",
        "eng_seq = eng_tokenizer.texts_to_sequences(eng_sentences)\n",
        "spa_seq = spa_tokenizer.texts_to_sequences(spa_sentences)\n",
        "\n",
        "max_eng_len = max(len(s) for s in eng_seq)\n",
        "max_spa_len = max(len(s) for s in spa_seq)\n",
        "\n",
        "eng_seq = pad_sequences(eng_seq, maxlen=max_eng_len, padding=\"post\")\n",
        "spa_seq = pad_sequences(spa_seq, maxlen=max_spa_len, padding=\"post\")\n",
        "\n",
        "eng_vocab = len(eng_tokenizer.word_index) + 1\n",
        "spa_vocab = len(spa_tokenizer.word_index) + 1\n",
        "embedding_dim = 64\n",
        "latent_dim = 128\n",
        "\n",
        "encoder_inputs = tf.keras.Input(shape=(max_eng_len,))\n",
        "enc_emb = Embedding(eng_vocab, embedding_dim)(encoder_inputs)\n",
        "\n",
        "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector\n",
        "decoder_inputs = tf.keras.Input(shape=(max_spa_len - 1,))\n",
        "dec_emb = Embedding(spa_vocab, embedding_dim)(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "attention = BahdanauAttention(latent_dim)\n",
        "\n",
        "outputs = []\n",
        "state_h_dec, state_c_dec = state_h, state_c\n",
        "\n",
        "for t in range(max_spa_len - 1):\n",
        "    context = attention(state_h_dec, encoder_outputs)\n",
        "    x = tf.concat([context[:, None, :], dec_emb[:, t:t+1, :]], axis=-1)\n",
        "    out, state_h_dec, state_c_dec = decoder_lstm(\n",
        "        x, initial_state=[state_h_dec, state_c_dec]\n",
        "    )\n",
        "    outputs.append(out)\n",
        "\n",
        "decoder_outputs = tf.concat(outputs, axis=1)\n",
        "decoder_dense = Dense(spa_vocab, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    [eng_seq, spa_seq[:, :-1]],\n",
        "    spa_seq[:, 1:],\n",
        "    epochs=300,\n",
        "    verbose=0\n",
        ")\n",
        "def translate(sentence):\n",
        "    seq = eng_tokenizer.texts_to_sequences([sentence])\n",
        "    seq = pad_sequences(seq, maxlen=max_eng_len, padding=\"post\")\n",
        "\n",
        "    enc_out, h, c = encoder_lstm(\n",
        "        Embedding(eng_vocab, embedding_dim)(seq)\n",
        "    )\n",
        "\n",
        "    target = spa_tokenizer.word_index[\"<start>\"]\n",
        "    result = []\n",
        "\n",
        "    for _ in range(max_spa_len):\n",
        "        context = attention(h, enc_out)\n",
        "        x = tf.concat([context[:, None, :],\n",
        "                       Embedding(spa_vocab, embedding_dim)(\n",
        "                           tf.constant([[target]])\n",
        "                       )], axis=-1)\n",
        "\n",
        "        out, h, c = decoder_lstm(x, initial_state=[h, c])\n",
        "        pred = tf.argmax(decoder_dense(out)[0, 0]).numpy()\n",
        "\n",
        "        word = spa_tokenizer.index_word.get(pred, \"\")\n",
        "        if word == \"<end>\":\n",
        "            break\n",
        "        result.append(word)\n",
        "        target = pred\n",
        "\n",
        "    return \" \".join(result)\n"
      ],
      "metadata": {
        "id": "fxYUTbAiMyRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OIKIoei3Nid-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9: Use the following short poetry dataset to simulate poem generation with a\n",
        "pre-trained GPT model:\n",
        "[\"Roses are red, violets are blue,\",\n",
        "\"Sugar is sweet, and so are you.\",\n",
        "\"The moon glows bright in silent skies,\",\n",
        "\"A bird sings where the soft wind sighs.\"]\n",
        "Using this dataset as a reference for poetic structure and language, generate a new 2-4\n",
        "line poem using a pre-trained GPT model (such as GPT-2). You may simulate\n",
        "fine-tuning by prompting the model with similar poetic patterns.\n",
        "Include your code, the prompt used, and the generated poem in your answer.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "5CA7tIRNNj3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 1. Imports\n",
        "# =========================\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# =========================\n",
        "# 2. Load Pre-trained GPT-2\n",
        "# =========================\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# =========================\n",
        "# 3. Prompt (Simulated Fine-Tuning)\n",
        "# =========================\n",
        "prompt = (\n",
        "    \"Roses are red, violets are blue,\\n\"\n",
        "    \"Sugar is sweet, and so are you.\\n\"\n",
        "    \"The moon glows bright in silent skies,\\n\"\n",
        "    \"A bird sings where the soft wind sighs.\\n\\n\"\n",
        "    \"Write a new short poem with 2 to 4 lines in a similar poetic style:\\n\"\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 4. Generate Poem\n",
        "# =========================\n",
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    max_length=120,\n",
        "    temperature=0.8,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"PROMPT USED:\\n\")\n",
        "print(prompt)\n",
        "print(\"\\nGENERATED POEM:\\n\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "8rglfM7LMyOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZdygxsEnNxWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 10: Imagine you are building a creative writing assistant for a publishing\n",
        "company. The assistant should generate story plots and character descriptions using\n",
        "Generative AI. Describe how you would design the system, including model selection,\n",
        "training data, bias mitigation, and evaluation methods. Explain the real-world challenges\n",
        "you might face.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "UTWmQytBNyEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Creative Writing Assistant Demo\n",
        "# =========================\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# Prompt for story plot\n",
        "plot_prompt = (\n",
        "    \"Generate a short story plot for a fantasy novel:\\n\"\n",
        "    \"Genre: Fantasy\\n\"\n",
        "    \"Tone: Mysterious\\n\"\n",
        "    \"Focus: Lost kingdom and hidden power\\n\\n\"\n",
        "    \"Story Plot:\\n\"\n",
        ")\n",
        "\n",
        "inputs = tokenizer.encode(plot_prompt, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    max_length=150,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "generated_plot = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== GENERATED STORY PLOT ===\\n\")\n",
        "print(generated_plot)\n",
        "\n",
        "# Prompt for character description\n",
        "character_prompt = (\n",
        "    \"Create a character description for a novel:\\n\"\n",
        "    \"Role: Protagonist\\n\"\n",
        "    \"Personality: Curious, brave, flawed\\n\"\n",
        "    \"Setting: Ancient fantasy world\\n\\n\"\n",
        "    \"Character Description:\\n\"\n",
        ")\n",
        "\n",
        "inputs = tokenizer.encode(character_prompt, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    max_length=140,\n",
        "    temperature=0.85,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "generated_character = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n=== GENERATED CHARACTER DESCRIPTION ===\\n\")\n",
        "print(generated_character)\n"
      ],
      "metadata": {
        "id": "4ntoQgkqNhy_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}